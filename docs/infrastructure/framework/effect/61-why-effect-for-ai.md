## Why Effect for AI?

Integrating LLMs isnâ€™t just about sending API requests â€” itâ€™s handling streaming output, retries, rate limits, timeouts, and user-driven side effects, all while keeping your system stable and responsive. Effect provides simple, composable building blocks to model these workflows in a **safe**, **declarative**, and **composable** manner.

By using Effect for your LLM interactions you'll benefit from:

- ğŸ§© **Provider-Agnostic Architecture**
  Write your business logic once, and defer choosing the underlying provider (OpenAI, Anthropic, local models, mocks, etc.) until runtime

- ğŸ§ª **Fully Testable**
  Because LLM interactions are modeled via Effect services, you can mock, simulate, or snapshot responses just by providing an alternative implementation

- ğŸ§µ **Structured Concurrency**
  Run concurrent LLM calls, cancel stale requests, stream partial results, or race multiple providers â€” all safely managed by Effectâ€™s structured concurrency model

- ğŸ” **Observability**
  Leverage Effect's built-in tracing, logging, and metrics to instrument your LLM interactions to gain deep insight into performance bottlenecks or failures in production

...and much more!
